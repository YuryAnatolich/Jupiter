{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Локальный Jupyter-пайплайн для пакетной обработки изображений\n",
    "\n",
    "Этот ноутбук реализует устойчивый к повторным запускам инкрементальный пайплайн для:\n",
    "- нормализации изображений в PNG,\n",
    "- переименования новых исходников,\n",
    "- сборки PDF по модулям,\n",
    "- добавления кликабельного оглавления в PDF (OCR + PyMuPDF),\n",
    "- сборки PPTX (2 изображения на слайд),\n",
    "- полной пересборки нормализации.\n",
    "\n",
    "Установка зависимостей:\n",
    "```bash\n",
    "pip install pillow natsort img2pdf pymupdf pandas pytesseract python-pptx tqdm\n",
    "```\n",
    "\n",
    "> Важно: Tesseract OCR должен быть установлен в Windows отдельно, а путь к `tesseract.exe` нужно указать в соответствующей ячейке OCR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1. Настройки и функции нормализации\n",
    "\n",
    "**Что делает:** задаёт базовый конфиг и функции нормализации изображения (автообрезка фона по цвету угла, вписывание в холст, сохранение PNG), а также функции обхода исходников и вычисления целевых путей.\n",
    "\n",
    "**Когда запускать:** в начале сессии и после любых изменений логики нормализации.\n",
    "\n",
    "**Зависимости:** нет. Это базовая ячейка для этапов 2–7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# ===== Конфиг этапа 1 =====\n",
    "SRC_ROOT = Path(r\"C:\\data\\screenshots\")\n",
    "OUT_ROOT = SRC_ROOT / \"_normalized\"\n",
    "TARGET_W = 1920\n",
    "TARGET_H = 1080\n",
    "TOLERANCE = 12\n",
    "LOG_PATH = OUT_ROOT / \"_normalize_log.csv\"\n",
    "PROBLEMS_PATH = OUT_ROOT / \"_normalize_problems.txt\"\n",
    "\n",
    "VALID_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _ts() -> str:\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def _append_problem(msg: str) -> None:\n",
    "    PROBLEMS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with PROBLEMS_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{_ts()}] {msg}\\n\")\n",
    "\n",
    "def autocrop_by_corner_bg(img: Image.Image, tol: int) -> Image.Image:\n",
    "    img_rgba = img.convert(\"RGBA\")\n",
    "    px = img_rgba.load()\n",
    "    w, h = img_rgba.size\n",
    "    bg = px[0, 0]\n",
    "\n",
    "    def is_bg(p):\n",
    "        return all(abs(int(p[i]) - int(bg[i])) <= tol for i in range(3))\n",
    "\n",
    "    left, top, right, bottom = w, h, -1, -1\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            if not is_bg(px[x, y]):\n",
    "                left = min(left, x)\n",
    "                top = min(top, y)\n",
    "                right = max(right, x)\n",
    "                bottom = max(bottom, y)\n",
    "\n",
    "    if right < left or bottom < top:\n",
    "        return img_rgba\n",
    "\n",
    "    return img_rgba.crop((left, top, right + 1, bottom + 1))\n",
    "\n",
    "def fit_to_canvas(img: Image.Image, target_w: int, target_h: int, bg=(255, 255, 255, 0)) -> Image.Image:\n",
    "    src = img.convert(\"RGBA\")\n",
    "    sw, sh = src.size\n",
    "    if sw == 0 or sh == 0:\n",
    "        raise ValueError(\"Пустое изображение\")\n",
    "\n",
    "    scale = min(target_w / sw, target_h / sh)\n",
    "    nw, nh = max(1, int(sw * scale)), max(1, int(sh * scale))\n",
    "    resized = src.resize((nw, nh), Image.Resampling.LANCZOS)\n",
    "\n",
    "    canvas = Image.new(\"RGBA\", (target_w, target_h), bg)\n",
    "    ox = (target_w - nw) // 2\n",
    "    oy = (target_h - nh) // 2\n",
    "    canvas.paste(resized, (ox, oy), resized)\n",
    "    return canvas\n",
    "\n",
    "def list_source_files() -> List[Path]:\n",
    "    files = [\n",
    "        p for p in SRC_ROOT.rglob(\"*\")\n",
    "        if p.is_file() and p.suffix.lower() in VALID_EXTS and OUT_ROOT not in p.parents\n",
    "    ]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def dst_path_for(src_path: Path) -> Path:\n",
    "    rel = src_path.relative_to(SRC_ROOT)\n",
    "    return (OUT_ROOT / rel).with_suffix(\".png\")\n",
    "\n",
    "def normalize_one(in_path: Path, out_path: Path) -> bool:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        with Image.open(in_path) as im:\n",
    "            cropped = autocrop_by_corner_bg(im, TOLERANCE)\n",
    "            fitted = fit_to_canvas(cropped, TARGET_W, TARGET_H)\n",
    "            fitted.save(out_path, format=\"PNG\", optimize=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        _append_problem(f\"normalize_one error: src={in_path} dst={out_path} err={e}\")\n",
    "        return False\n",
    "\n",
    "print(f\"[ok] База инициализирована. SRC_ROOT={SRC_ROOT}\")\n",
    "print(f\"[info] Выход нормализации: {OUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2. Переименование только новых исходных файлов\n",
    "\n",
    "**Что делает:** переименовывает только файлы, которые ещё не имеют имени формата `NNN.ext` внутри каждой папки, продолжая нумерацию от максимального номера.\n",
    "\n",
    "**Когда запускать:** перед нормализацией, если хотите унифицировать имена новых файлов.\n",
    "\n",
    "**Зависимости:** этап 1 (конфиг `SRC_ROOT`, `VALID_EXTS`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Конфиг этапа 2 =====\n",
    "DRY_RUN = True\n",
    "\n",
    "num_re = re.compile(r\"^(\\d{3,})$\")\n",
    "\n",
    "def rename_new_files(dry_run: bool = True):\n",
    "    stats = {\"folders\": 0, \"planned\": 0, \"renamed\": 0, \"skipped_numbered\": 0, \"errors\": 0}\n",
    "    plans = []\n",
    "\n",
    "    for folder in sorted([p for p in SRC_ROOT.rglob(\"*\") if p.is_dir() and OUT_ROOT not in p.parents and p != OUT_ROOT]):\n",
    "        files = sorted([f for f in folder.iterdir() if f.is_file() and f.suffix.lower() in VALID_EXTS])\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        stats[\"folders\"] += 1\n",
    "        max_num = 0\n",
    "        pending = []\n",
    "        for f in files:\n",
    "            stem = f.stem\n",
    "            m = num_re.match(stem)\n",
    "            if m:\n",
    "                max_num = max(max_num, int(m.group(1)))\n",
    "                stats[\"skipped_numbered\"] += 1\n",
    "            else:\n",
    "                pending.append(f)\n",
    "\n",
    "        next_num = max_num + 1\n",
    "        for src in pending:\n",
    "            dst = src.with_name(f\"{next_num:03d}{src.suffix.lower()}\")\n",
    "            while dst.exists():\n",
    "                next_num += 1\n",
    "                dst = src.with_name(f\"{next_num:03d}{src.suffix.lower()}\")\n",
    "            plans.append((src, dst))\n",
    "            next_num += 1\n",
    "\n",
    "    stats[\"planned\"] = len(plans)\n",
    "    print(f\"[info] План переименования: {len(plans)} файлов\")\n",
    "    for src, dst in plans:\n",
    "        print(f\"[build] {src} -> {dst.name}\")\n",
    "\n",
    "    if dry_run:\n",
    "        print(\"[skip] DRY_RUN=True, файловая система не изменена\")\n",
    "    else:\n",
    "        for src, dst in plans:\n",
    "            try:\n",
    "                src.rename(dst)\n",
    "                stats[\"renamed\"] += 1\n",
    "                print(f\"[ok] renamed: {src.name} -> {dst.name}\")\n",
    "            except Exception as e:\n",
    "                stats[\"errors\"] += 1\n",
    "                _append_problem(f\"rename error: {src} -> {dst}, err={e}\")\n",
    "                print(f\"[error] {src} -> {dst}: {e}\")\n",
    "\n",
    "    print(\"[info] Summary rename:\", stats)\n",
    "    return stats\n",
    "\n",
    "rename_stats = rename_new_files(dry_run=DRY_RUN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 3. Инкрементальная нормализация (mtime)\n",
    "\n",
    "**Что делает:** обрабатывает только новые/изменённые исходники, ведёт append-лог в CSV и append-лог проблем в TXT.\n",
    "\n",
    "**Когда запускать:** после этапа 1, обычно после этапа 2 (если было переименование).\n",
    "\n",
    "**Зависимости:** этап 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Конфиг этапа 3 =====\n",
    "FORCE_REBUILD = False\n",
    "\n",
    "def needs_processing(src: Path, dst: Path) -> bool:\n",
    "    if not dst.exists():\n",
    "        return True\n",
    "    return src.stat().st_mtime > dst.stat().st_mtime\n",
    "\n",
    "def ensure_csv_header(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"ts\", \"src\", \"dst\", \"status\", \"elapsed_sec\", \"error\"])\n",
    "\n",
    "def append_csv_row(path: Path, row: list):\n",
    "    with path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow(row)\n",
    "\n",
    "def run_incremental_normalize(force_rebuild: bool = False):\n",
    "    started = time.perf_counter()\n",
    "    ensure_csv_header(LOG_PATH)\n",
    "\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    problems = 0\n",
    "\n",
    "    for src in list_source_files():\n",
    "        dst = dst_path_for(src)\n",
    "        if (not force_rebuild) and (not needs_processing(src, dst)):\n",
    "            skipped += 1\n",
    "            print(f\"[skip] up-to-date: {src}\")\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        ok = normalize_one(src, dst)\n",
    "        dt = round(time.perf_counter() - t0, 4)\n",
    "        if ok:\n",
    "            processed += 1\n",
    "            append_csv_row(LOG_PATH, [_ts(), str(src), str(dst), \"ok\", dt, \"\"])\n",
    "            print(f\"[ok] normalized: {src} -> {dst}\")\n",
    "        else:\n",
    "            problems += 1\n",
    "            append_csv_row(LOG_PATH, [_ts(), str(src), str(dst), \"error\", dt, \"normalize failed\"])\n",
    "            print(f\"[error] normalize failed: {src}\")\n",
    "\n",
    "    total = round(time.perf_counter() - started, 2)\n",
    "    print(f\"[info] Summary normalize: processed={processed}, skipped={skipped}, problems={problems}, time={total}s\")\n",
    "\n",
    "run_incremental_normalize(force_rebuild=FORCE_REBUILD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 4. Сборка PDF по модулям из `_normalized`\n",
    "\n",
    "**Что делает:** формирует PDF по подпапкам-модулям (`000__intro` идёт первой), с natural sort, инкрементальным пропуском актуальных файлов, и fallback `img2pdf -> PIL`.\n",
    "\n",
    "**Когда запускать:** после нормализации (этап 3 или 7).\n",
    "\n",
    "**Зависимости:** этап 1 (структура и пути нормализованных PNG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "import img2pdf\n",
    "\n",
    "# ===== Конфиг этапа 4 =====\n",
    "PDF_OUT_ROOT = SRC_ROOT / \"_pdf_modules\"\n",
    "INCREMENTAL = True\n",
    "PDF_OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def module_dirs(root: Path):\n",
    "    subs = [d for d in root.iterdir() if d.is_dir()] if root.exists() else []\n",
    "    if not subs:\n",
    "        return [(\"all\", root)]\n",
    "\n",
    "    def key(d: Path):\n",
    "        return (0 if d.name.lower() == \"000__intro\" else 1, d.name.lower())\n",
    "\n",
    "    ordered = sorted(subs, key=key)\n",
    "    return [(d.name, d) for d in ordered]\n",
    "\n",
    "def module_images(mod_path: Path):\n",
    "    files = [p for p in mod_path.rglob(\"*.png\") if p.is_file()]\n",
    "    return natsorted(files, key=lambda x: str(x.relative_to(mod_path)).lower())\n",
    "\n",
    "def _latest_mtime(paths):\n",
    "    return max((p.stat().st_mtime for p in paths), default=0)\n",
    "\n",
    "def build_pdf(images: list[Path], out_pdf: Path):\n",
    "    out_pdf.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        with out_pdf.open(\"wb\") as f:\n",
    "            f.write(img2pdf.convert([str(p) for p in images]))\n",
    "        return \"img2pdf\"\n",
    "    except Exception:\n",
    "        pil_images = []\n",
    "        try:\n",
    "            for p in images:\n",
    "                pil_images.append(Image.open(p).convert(\"RGB\"))\n",
    "            first, rest = pil_images[0], pil_images[1:]\n",
    "            first.save(out_pdf, save_all=True, append_images=rest)\n",
    "            return \"PIL\"\n",
    "        finally:\n",
    "            for im in pil_images:\n",
    "                im.close()\n",
    "\n",
    "def run_pdf_build(incremental: bool = True):\n",
    "    for mod_name, mod_dir in module_dirs(OUT_ROOT):\n",
    "        images = module_images(mod_dir)\n",
    "        if not images:\n",
    "            print(f\"[skip] module={mod_name}: нет PNG\")\n",
    "            continue\n",
    "\n",
    "        out_pdf = PDF_OUT_ROOT / f\"{mod_name}.pdf\"\n",
    "        if incremental and out_pdf.exists() and out_pdf.stat().st_mtime >= _latest_mtime(images):\n",
    "            print(f\"[skip] module={mod_name}: PDF актуален\")\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            engine = build_pdf(images, out_pdf)\n",
    "            dt = round(time.perf_counter() - t0, 2)\n",
    "            print(f\"[ok] module={mod_name} pages={len(images)} engine={engine} time={dt}s\")\n",
    "            print(f\"[info] first={images[0]}\")\n",
    "            print(f\"[info] last={images[-1]}\")\n",
    "        except Exception as e:\n",
    "            _append_problem(f\"pdf build error module={mod_name}: {e}\\n{traceback.format_exc()}\")\n",
    "            print(f\"[error] module={mod_name}: {e}\")\n",
    "\n",
    "run_pdf_build(incremental=INCREMENTAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 5. Кликабельное оглавление в PDF (OCR + PyMuPDF)\n",
    "\n",
    "**Что делает:** ищет страницы оглавления через OCR (`pytesseract + pandas`), извлекает пункты с номерами страниц справа, и вставляет кликабельные ссылки `fitz.LINK_GOTO` в новый файл `*_toc.pdf`.\n",
    "\n",
    "**Когда запускать:** после этапа 4, если нужно интерактивное оглавление в PDF.\n",
    "\n",
    "**Зависимости:** этап 4 (готовые `*_pdf_modules/*.pdf`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "\n",
    "# ===== Конфиг этапа 5 =====\n",
    "TESSERACT_EXE = Path(r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\")\n",
    "OCR_LANG = \"rus+eng\"\n",
    "OCR_DPI = 220\n",
    "PDF_GLOB = \"*.pdf\"\n",
    "TOC_LOG_PATH = PDF_OUT_ROOT / \"_toc_log.csv\"\n",
    "TOC_INCREMENTAL = True\n",
    "\n",
    "if TESSERACT_EXE.exists():\n",
    "    pytesseract.pytesseract.tesseract_cmd = str(TESSERACT_EXE)\n",
    "    print(f\"[ok] tesseract path set: {TESSERACT_EXE}\")\n",
    "else:\n",
    "    print(f\"[error] Не найден tesseract.exe: {TESSERACT_EXE}\")\n",
    "\n",
    "def render_page_for_ocr(doc: fitz.Document, page_index: int, dpi: int = 220) -> Image.Image:\n",
    "    page = doc[page_index]\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    return Image.open(io.BytesIO(pix.tobytes(\"png\"))).convert(\"RGB\")\n",
    "\n",
    "def _clean_ocr_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in [\"left\", \"top\", \"width\", \"height\", \"conf\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "    df = df[(df[\"text\"] != \"\") & (df[\"conf\"].fillna(-1) >= 0)].copy()\n",
    "    return df\n",
    "\n",
    "def _extract_right_side_numbers(df: pd.DataFrame, page_w: int) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    df2[\"is_num\"] = df2[\"text\"].str.fullmatch(r\"\\d{1,4}\")\n",
    "    right_zone = df2[\"left\"] > (page_w * 0.62)\n",
    "    return df2[df2[\"is_num\"] & right_zone].copy()\n",
    "\n",
    "def _contains_toc_keyword(text: str) -> bool:\n",
    "    t = text.lower().replace(\"ё\", \"е\")\n",
    "    keys = [\"оглавлен\", \"содержан\", \"содержаиие\", \"огпавление\", \"coдepжaниe\"]\n",
    "    return any(k in t for k in keys)\n",
    "\n",
    "def detect_toc_pages_and_entries(pdf_path: Path, max_scan_pages: int = 12):\n",
    "    entries = []\n",
    "    toc_pages = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        scan_pages = min(len(doc), max_scan_pages)\n",
    "        for i in range(scan_pages):\n",
    "            image = render_page_for_ocr(doc, i, dpi=OCR_DPI)\n",
    "            data = pytesseract.image_to_data(image, lang=OCR_LANG, output_type=pytesseract.Output.DATAFRAME)\n",
    "            df = _clean_ocr_df(data)\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            full_text = \" \".join(df[\"text\"].tolist())\n",
    "            num_df = _extract_right_side_numbers(df, image.width)\n",
    "\n",
    "            soft_threshold = 1 if i < 3 else 2\n",
    "            if _contains_toc_keyword(full_text) or len(num_df) >= soft_threshold:\n",
    "                toc_pages.append(i)\n",
    "\n",
    "                lines = {}\n",
    "                for _, r in df.iterrows():\n",
    "                    key = int(round(float(r[\"top\"]) / 8.0) * 8)\n",
    "                    lines.setdefault(key, []).append(r)\n",
    "\n",
    "                for y, rows in sorted(lines.items(), key=lambda x: x[0]):\n",
    "                    rows = sorted(rows, key=lambda r: float(r[\"left\"]))\n",
    "                    texts = [str(r[\"text\"]) for r in rows]\n",
    "                    if len(texts) < 2:\n",
    "                        continue\n",
    "\n",
    "                    last = texts[-1]\n",
    "                    if not re.fullmatch(r\"\\d{1,4}\", last):\n",
    "                        continue\n",
    "\n",
    "                    title = \" \".join(texts[:-1]).strip(\" .•·-—_\")\n",
    "                    if len(title) < 2:\n",
    "                        continue\n",
    "\n",
    "                    src_page_1based = int(last)\n",
    "                    left_vals = [float(r[\"left\"]) for r in rows]\n",
    "                    top_vals = [float(r[\"top\"]) for r in rows]\n",
    "                    width_vals = [float(r[\"width\"]) for r in rows]\n",
    "                    height_vals = [float(r[\"height\"]) for r in rows]\n",
    "                    x0 = min(left_vals)\n",
    "                    y0 = min(top_vals)\n",
    "                    x1 = max(l + w for l, w in zip(left_vals, width_vals))\n",
    "                    y1 = max(t + h for t, h in zip(top_vals, height_vals))\n",
    "\n",
    "                    entries.append({\n",
    "                        \"toc_page\": i,\n",
    "                        \"title\": title,\n",
    "                        \"target_page_1based\": src_page_1based,\n",
    "                        \"bbox\": (x0, y0, x1, y1),\n",
    "                    })\n",
    "\n",
    "        uniq = []\n",
    "        seen = set()\n",
    "        for e in entries:\n",
    "            key = (e[\"toc_page\"], e[\"title\"].lower(), e[\"target_page_1based\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                uniq.append(e)\n",
    "\n",
    "        return toc_pages, uniq\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def add_toc_links(in_pdf: Path, out_pdf: Path, entries: list[dict]):\n",
    "    doc = fitz.open(in_pdf)\n",
    "    links_added = 0\n",
    "    try:\n",
    "        page_count = len(doc)\n",
    "        for e in entries:\n",
    "            src_i = int(e[\"toc_page\"])\n",
    "            dst_i = int(e[\"target_page_1based\"]) - 1\n",
    "            if src_i < 0 or src_i >= page_count or dst_i < 0 or dst_i >= page_count:\n",
    "                continue\n",
    "\n",
    "            x0, y0, x1, y1 = e[\"bbox\"]\n",
    "            rect = fitz.Rect(float(x0), float(y0), float(x1), float(y1))\n",
    "            link = {\n",
    "                \"kind\": fitz.LINK_GOTO,\n",
    "                \"from\": rect,\n",
    "                \"page\": dst_i,\n",
    "                \"zoom\": 0,\n",
    "            }\n",
    "            doc[src_i].insert_link(link)\n",
    "            links_added += 1\n",
    "\n",
    "        out_pdf.parent.mkdir(parents=True, exist_ok=True)\n",
    "        doc.save(out_pdf)\n",
    "        return links_added\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def ensure_toc_log_header(path: Path):\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow([\"ts\", \"pdf\", \"out\", \"status\", \"toc_pages\", \"entries\", \"links\", \"error\"])\n",
    "\n",
    "def run_toc_ocr_incremental():\n",
    "    ensure_toc_log_header(TOC_LOG_PATH)\n",
    "    done = skipped = no_toc = no_links = errors = 0\n",
    "\n",
    "    pdfs = [p for p in PDF_OUT_ROOT.glob(PDF_GLOB) if p.is_file() and not p.name.endswith(\"_toc.pdf\")]\n",
    "    pdfs = natsorted(pdfs, key=lambda p: p.name.lower())\n",
    "\n",
    "    for pdf in pdfs:\n",
    "        out = pdf.with_name(f\"{pdf.stem}_toc.pdf\")\n",
    "        if TOC_INCREMENTAL and out.exists() and out.stat().st_mtime >= pdf.stat().st_mtime:\n",
    "            skipped += 1\n",
    "            print(f\"[skip] TOC актуален: {pdf.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            toc_pages, entries = detect_toc_pages_and_entries(pdf)\n",
    "            if not toc_pages:\n",
    "                no_toc += 1\n",
    "                with TOC_LOG_PATH.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    csv.writer(f).writerow([_ts(), str(pdf), str(out), \"no_toc\", 0, 0, 0, \"\"])\n",
    "                print(f\"[skip] TOC не найден: {pdf.name}\")\n",
    "                continue\n",
    "\n",
    "            links = add_toc_links(pdf, out, entries)\n",
    "            if links == 0:\n",
    "                no_links += 1\n",
    "                status = \"no_links\"\n",
    "                print(f\"[skip] TOC найден, но ссылок нет: {pdf.name}\")\n",
    "            else:\n",
    "                done += 1\n",
    "                status = \"done\"\n",
    "                print(f\"[ok] TOC добавлен: {pdf.name} -> {out.name}, links={links}\")\n",
    "\n",
    "            with TOC_LOG_PATH.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                csv.writer(f).writerow([_ts(), str(pdf), str(out), status, len(toc_pages), len(entries), links, \"\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            err = f\"{e}\"\n",
    "            _append_problem(f\"toc error for {pdf}: {e}\\n{traceback.format_exc()}\")\n",
    "            with TOC_LOG_PATH.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                csv.writer(f).writerow([_ts(), str(pdf), str(out), \"error\", 0, 0, 0, err])\n",
    "            print(f\"[error] TOC ошибка: {pdf.name}: {e}\")\n",
    "\n",
    "    print(f\"[info] Summary TOC: done={done}, skipped={skipped}, no_toc={no_toc}, no_links={no_links}, errors={errors}\")\n",
    "\n",
    "run_toc_ocr_incremental()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 6. Сборка PPTX по модулям (2-up)\n",
    "\n",
    "**Что делает:** создаёт презентации по модулям: титульный слайд и далее по 2 изображения на слайд. Есть инкрементальный пропуск по mtime.\n",
    "\n",
    "**Когда запускать:** после нормализации (этап 3 или 7).\n",
    "\n",
    "**Зависимости:** этап 1 (нормализованные PNG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "\n",
    "# ===== Конфиг этапа 6 =====\n",
    "PPTX_OUT_ROOT = SRC_ROOT / \"_pptx_modules\"\n",
    "PPTX_INCREMENTAL = True\n",
    "PPTX_OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_pptx_2up(images: list[Path], out_pptx: Path, title: str):\n",
    "    prs = Presentation()\n",
    "\n",
    "    title_slide = prs.slides.add_slide(prs.slide_layouts[5])\n",
    "    if title_slide.shapes.title is not None:\n",
    "        title_slide.shapes.title.text = title\n",
    "\n",
    "    slide_w = prs.slide_width.inches\n",
    "    slide_h = prs.slide_height.inches\n",
    "    margin = 0.3\n",
    "    gap = 0.2\n",
    "\n",
    "    box_w = slide_w - 2 * margin\n",
    "    box_h = (slide_h - 2 * margin - gap) / 2\n",
    "\n",
    "    for i in range(0, len(images), 2):\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "        chunk = images[i:i+2]\n",
    "        for j, img_path in enumerate(chunk):\n",
    "            top = margin + j * (box_h + gap)\n",
    "            slide.shapes.add_picture(str(img_path), Inches(margin), Inches(top), width=Inches(box_w), height=Inches(box_h))\n",
    "\n",
    "        note = slide.shapes.add_textbox(Inches(margin), Inches(slide_h - 0.35), Inches(box_w), Inches(0.25))\n",
    "        tf = note.text_frame\n",
    "        tf.text = f\"{chunk[0].name}\" + (f\" | {chunk[1].name}\" if len(chunk) > 1 else \"\")\n",
    "        tf.paragraphs[0].font.size = Pt(10)\n",
    "\n",
    "    out_pptx.parent.mkdir(parents=True, exist_ok=True)\n",
    "    prs.save(out_pptx)\n",
    "\n",
    "def run_pptx_build(incremental: bool = True):\n",
    "    for mod_name, mod_dir in module_dirs(OUT_ROOT):\n",
    "        images = module_images(mod_dir)\n",
    "        if not images:\n",
    "            print(f\"[skip] module={mod_name}: нет PNG\")\n",
    "            continue\n",
    "\n",
    "        out_pptx = PPTX_OUT_ROOT / f\"{mod_name}.pptx\"\n",
    "        if incremental and out_pptx.exists() and out_pptx.stat().st_mtime >= _latest_mtime(images):\n",
    "            print(f\"[skip] module={mod_name}: PPTX актуален\")\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            build_pptx_2up(images, out_pptx, title=f\"Модуль: {mod_name}\")\n",
    "            dt = round(time.perf_counter() - t0, 2)\n",
    "            slides = 1 + (len(images) + 1) // 2\n",
    "            print(f\"[ok] module={mod_name} pages={len(images)} slides={slides} time={dt}s\")\n",
    "            print(f\"[info] first={images[0]}\")\n",
    "            print(f\"[info] last={images[-1]}\")\n",
    "        except Exception as e:\n",
    "            _append_problem(f\"pptx build error module={mod_name}: {e}\\n{traceback.format_exc()}\")\n",
    "            print(f\"[error] module={mod_name}: {e}\")\n",
    "\n",
    "run_pptx_build(incremental=PPTX_INCREMENTAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 7. Полная пересборка нормализации\n",
    "\n",
    "**Что делает:** пересобирает все нормализованные PNG заново для всех исходников с прогресс-баром `tqdm`, полностью перезаписывая лог CSV и TXT проблем.\n",
    "\n",
    "**Когда запускать:** когда нужно гарантированно пересоздать весь набор нормализованных данных (например, после изменения `TARGET_W/TARGET_H/TOLERANCE`).\n",
    "\n",
    "**Зависимости:** этап 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ===== Конфиг этапа 7 =====\n",
    "FULL_REBUILD = False\n",
    "\n",
    "def run_full_rebuild(enabled: bool = False):\n",
    "    if not enabled:\n",
    "        print(\"[skip] FULL_REBUILD=False, полная пересборка не запущена\")\n",
    "        return\n",
    "\n",
    "    started = time.perf_counter()\n",
    "    files = list_source_files()\n",
    "\n",
    "    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LOG_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow([\"ts\", \"src\", \"dst\", \"status\", \"elapsed_sec\", \"error\"])\n",
    "    PROBLEMS_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    processed = 0\n",
    "    problems = 0\n",
    "\n",
    "    for src in tqdm(files, desc=\"Full normalize\", unit=\"img\"):\n",
    "        dst = dst_path_for(src)\n",
    "        t0 = time.perf_counter()\n",
    "        ok = normalize_one(src, dst)\n",
    "        dt = round(time.perf_counter() - t0, 4)\n",
    "\n",
    "        if ok:\n",
    "            processed += 1\n",
    "            append_csv_row(LOG_PATH, [_ts(), str(src), str(dst), \"ok\", dt, \"\"])\n",
    "        else:\n",
    "            problems += 1\n",
    "            append_csv_row(LOG_PATH, [_ts(), str(src), str(dst), \"error\", dt, \"normalize failed\"])\n",
    "\n",
    "    total = round(time.perf_counter() - started, 2)\n",
    "    print(f\"[info] Summary full rebuild: total={len(files)}, processed={processed}, problems={problems}, time={total}s\")\n",
    "\n",
    "run_full_rebuild(enabled=FULL_REBUILD)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}